{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = \"VOCtrainval_06-Nov-2007\\VOCdevkit\\VOC2007\\JPEGImages\"\n",
    "mask_directory = \"VOCtrainval_06-Nov-2007\\VOCdevkit\\VOC2007\\SegmentationClass\"\n",
    "batch_size = 32\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a45580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义U-Net模型中的编码器部分\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        skip_connection = x\n",
    "        x = self.maxpool(x)\n",
    "        return x, skip_connection\n",
    "\n",
    "# 定义U-Net模型中的解码器部分\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# 定义完整的 U-Net 模型\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = UNetEncoder(in_channels, 64)\n",
    "        self.encoder2 = UNetEncoder(64, 128)\n",
    "        self.encoder3 = UNetEncoder(128, 256)\n",
    "        self.encoder4 = UNetEncoder(256, 512)\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.decoder4 = UNetDecoder(1024, 256)\n",
    "        self.decoder3 = UNetDecoder(512, 128)\n",
    "        self.decoder2 = UNetDecoder(256, 64)\n",
    "        self.decoder1 = UNetDecoder(128, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "        x, skip3 = self.encoder3(x)\n",
    "        x, skip4 = self.encoder4(x)\n",
    "        x = self.center(x)\n",
    "        x = self.decoder4(x, skip4)\n",
    "        x = self.decoder3(x, skip3)\n",
    "        x = self.decoder2(x, skip2)\n",
    "        x = self.decoder1(x, skip1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961afd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.mask_files = os.listdir(mask_dir)\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[index])\n",
    "        image = self.transform(Image.open(image_path))\n",
    "        mask = self.transform(Image.open(mask_path))\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11597760",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()  # 图像转为Tensor\n",
    "dataset = CustomDataset(image_directory, mask_directory, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # 将图像和遮罩图像输入生成器，生成合成图像\n",
    "        synthetic_images = generator(images)\n",
    "\n",
    "        # 将真实图像和合成图像与相应的遮罩图像一起输入给判别器进行判断\n",
    "        real_outputs = discriminator(images, masks)\n",
    "        synthetic_outputs = discriminator(synthetic_images, masks)\n",
    "\n",
    "        # 计算生成器和判别器的损失\n",
    "        generator_loss = criterion(synthetic_outputs, torch.ones_like(synthetic_outputs))\n",
    "        discriminator_loss = criterion(real_outputs, torch.ones_like(real_outputs)) + criterion(synthetic_outputs, torch.zeros_like(synthetic_outputs))\n",
    "\n",
    "        # 反向传播和更新生成器的参数\n",
    "        generator_optimizer.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # 反向传播和更新判别器的参数\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "    # 打印当前训练轮数和损失\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: {generator_loss.item()}, Discriminator Loss: {discriminator_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23bf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20010824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
